#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAGç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–å™¨
ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½ï¼Œç¡®ä¿å“åº”æ—¶é—´<2ç§’ï¼Œæå‡èµ„æºåˆ©ç”¨ç‡
"""

import os
import sys
import logging
import time
import gc
import psutil
import torch
from typing import Dict, Any, List, Optional
import numpy as np
from concurrent.futures import ThreadPoolExecutor
import threading

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from configs.rag_config import RAGConfig

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self, config: RAGConfig = None):
        """
        åˆå§‹åŒ–æ€§èƒ½ä¼˜åŒ–å™¨
        
        Args:
            config: RAGé…ç½®å¯¹è±¡
        """
        self.config = config or RAGConfig()
        self.optimization_cache = {}
        self.performance_metrics = {}
        self._lock = threading.Lock()
        
        # æ£€æµ‹ç¡¬ä»¶é…ç½®
        self.hardware_info = self._detect_hardware()
        logging.info(f"ç¡¬ä»¶é…ç½®: {self.hardware_info}")
    
    def _detect_hardware(self) -> Dict[str, Any]:
        """
        æ£€æµ‹ç¡¬ä»¶é…ç½®
        
        Returns:
            ç¡¬ä»¶ä¿¡æ¯
        """
        hardware_info = {
            'cpu_count': psutil.cpu_count(),
            'memory_total': psutil.virtual_memory().total / (1024**3),  # GB
            'memory_available': psutil.virtual_memory().available / (1024**3),  # GB
            'gpu_available': torch.cuda.is_available(),
            'gpu_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'gpu_memory': []
        }
        
        # GPUä¿¡æ¯
        if hardware_info['gpu_available']:
            for i in range(hardware_info['gpu_count']):
                gpu_props = torch.cuda.get_device_properties(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)  # GB
                hardware_info['gpu_memory'].append({
                    'device': i,
                    'name': gpu_props.name,
                    'memory_total': gpu_memory,
                    'memory_free': torch.cuda.memory_reserved(i) / (1024**3) if torch.cuda.is_available() else 0
                })
        
        return hardware_info
    
    def optimize_pytorch_settings(self) -> Dict[str, Any]:
        """
        ä¼˜åŒ–PyTorchè®¾ç½®
        
        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        logging.info("ä¼˜åŒ–PyTorchè®¾ç½®...")
        
        optimizations = {}
        
        try:
            # è®¾ç½®çº¿ç¨‹æ•°
            if self.hardware_info['cpu_count'] >= 4:
                torch.set_num_threads(min(4, self.hardware_info['cpu_count'] // 2))
                optimizations['num_threads'] = torch.get_num_threads()
            
            # å¯ç”¨ä¼˜åŒ–
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            optimizations['cudnn_benchmark'] = True
            
            # GPUå†…å­˜ä¼˜åŒ–
            if self.hardware_info['gpu_available']:
                # å¯ç”¨å†…å­˜æ± 
                os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
                
                # è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥
                torch.cuda.empty_cache()
                optimizations['gpu_memory_optimized'] = True
                
                # æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨
                for i, gpu_info in enumerate(self.hardware_info['gpu_memory']):
                    if gpu_info['memory_total'] <= 8.5:  # RTX4060 8GBé™åˆ¶
                        # ä¸º8GBæ˜¾å­˜ä¼˜åŒ–
                        torch.cuda.set_per_process_memory_fraction(0.85, device=i)
                        optimizations[f'gpu_{i}_memory_fraction'] = 0.85
            
            # è®¾ç½®æ•°æ®ç±»å‹ä¼˜åŒ–
            torch.set_default_dtype(torch.float32)
            optimizations['default_dtype'] = 'float32'
            
            logging.info(f"PyTorchä¼˜åŒ–å®Œæˆ: {optimizations}")
            return optimizations
            
        except Exception as e:
            logging.error(f"PyTorchä¼˜åŒ–å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def optimize_embedding_model(self, model_path: str) -> Dict[str, Any]:
        """
        ä¼˜åŒ–åµŒå…¥æ¨¡å‹åŠ è½½å’Œæ¨ç†
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            
        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        logging.info("ä¼˜åŒ–åµŒå…¥æ¨¡å‹...")
        
        optimizations = {}
        
        try:
            # æ¨¡å‹é‡åŒ–å»ºè®®
            if self.hardware_info['gpu_available']:
                gpu_memory = self.hardware_info['gpu_memory'][0]['memory_total']
                if gpu_memory <= 8.5:  # RTX4060é™åˆ¶
                    optimizations['quantization_recommended'] = True
                    optimizations['precision'] = 'fp16'
                    optimizations['batch_size_limit'] = 32
                else:
                    optimizations['precision'] = 'fp32'
                    optimizations['batch_size_limit'] = 64
            else:
                optimizations['precision'] = 'fp32'
                optimizations['batch_size_limit'] = 16
            
            # ç¼“å­˜ç­–ç•¥
            optimizations['model_cache'] = True
            optimizations['embedding_cache_size'] = min(1000, int(self.hardware_info['memory_available'] * 100))
            
            # æ‰¹å¤„ç†ä¼˜åŒ–
            optimizations['dynamic_batching'] = True
            optimizations['max_batch_size'] = optimizations['batch_size_limit']
            
            logging.info(f"åµŒå…¥æ¨¡å‹ä¼˜åŒ–å®Œæˆ: {optimizations}")
            return optimizations
            
        except Exception as e:
            logging.error(f"åµŒå…¥æ¨¡å‹ä¼˜åŒ–å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def optimize_vector_store(self) -> Dict[str, Any]:
        """
        ä¼˜åŒ–å‘é‡å­˜å‚¨
        
        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        logging.info("ä¼˜åŒ–å‘é‡å­˜å‚¨...")
        
        optimizations = {}
        
        try:
            # ChromaDBä¼˜åŒ–è®¾ç½®
            optimizations['chroma_settings'] = {
                'anonymized_telemetry': False,
                'allow_reset': True,
                'is_persistent': True
            }
            
            # ç´¢å¼•ä¼˜åŒ–
            optimizations['index_optimization'] = {
                'hnsw_space': 'cosine',
                'hnsw_construction_ef': 200,
                'hnsw_m': 16,
                'hnsw_ef_search': 100
            }
            
            # æ‰¹é‡æ“ä½œä¼˜åŒ–
            optimizations['batch_size'] = min(100, int(self.hardware_info['memory_available'] * 10))
            optimizations['parallel_processing'] = min(4, self.hardware_info['cpu_count'])
            
            # ç¼“å­˜ç­–ç•¥
            optimizations['query_cache_size'] = 500
            optimizations['result_cache_ttl'] = 3600  # 1å°æ—¶
            
            logging.info(f"å‘é‡å­˜å‚¨ä¼˜åŒ–å®Œæˆ: {optimizations}")
            return optimizations
            
        except Exception as e:
            logging.error(f"å‘é‡å­˜å‚¨ä¼˜åŒ–å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def optimize_text_processing(self) -> Dict[str, Any]:
        """
        ä¼˜åŒ–æ–‡æœ¬å¤„ç†
        
        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        logging.info("ä¼˜åŒ–æ–‡æœ¬å¤„ç†...")
        
        optimizations = {}
        
        try:
            # åˆ†å—ç­–ç•¥ä¼˜åŒ–
            memory_gb = self.hardware_info['memory_available']
            
            if memory_gb >= 20:
                chunk_size = 1000
                overlap = 150
                batch_size = 50
            elif memory_gb >= 15:
                chunk_size = 800
                overlap = 120
                batch_size = 30
            else:
                chunk_size = 600
                overlap = 100
                batch_size = 20
            
            optimizations['chunk_size'] = chunk_size
            optimizations['chunk_overlap'] = overlap
            optimizations['processing_batch_size'] = batch_size
            
            # å¹¶è¡Œå¤„ç†
            optimizations['parallel_workers'] = min(4, self.hardware_info['cpu_count'])
            optimizations['use_multiprocessing'] = self.hardware_info['cpu_count'] >= 4
            
            # ä¸­è‹±æ–‡å¤„ç†ä¼˜åŒ–
            optimizations['mixed_language_support'] = True
            optimizations['unicode_normalization'] = True
            optimizations['text_cleaning'] = True
            
            logging.info(f"æ–‡æœ¬å¤„ç†ä¼˜åŒ–å®Œæˆ: {optimizations}")
            return optimizations
            
        except Exception as e:
            logging.error(f"æ–‡æœ¬å¤„ç†ä¼˜åŒ–å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def optimize_memory_usage(self) -> Dict[str, Any]:
        """
        ä¼˜åŒ–å†…å­˜ä½¿ç”¨
        
        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        logging.info("ä¼˜åŒ–å†…å­˜ä½¿ç”¨...")
        
        optimizations = {}
        
        try:
            # åƒåœ¾å›æ”¶ä¼˜åŒ–
            gc.collect()
            if self.hardware_info['gpu_available']:
                torch.cuda.empty_cache()
            
            # å†…å­˜ç›‘æ§
            memory_info = psutil.virtual_memory()
            optimizations['memory_before'] = {
                'total': memory_info.total / (1024**3),
                'available': memory_info.available / (1024**3),
                'percent': memory_info.percent
            }
            
            # è®¾ç½®å†…å­˜é™åˆ¶
            available_memory = memory_info.available / (1024**3)
            if available_memory < 10:  # å°äº10GBå¯ç”¨å†…å­˜
                optimizations['memory_limit'] = available_memory * 0.7
                optimizations['conservative_mode'] = True
            else:
                optimizations['memory_limit'] = available_memory * 0.8
                optimizations['conservative_mode'] = False
            
            # GPUå†…å­˜ä¼˜åŒ–
            if self.hardware_info['gpu_available']:
                for i in range(self.hardware_info['gpu_count']):
                    torch.cuda.empty_cache()
                    gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
                    if gpu_memory <= 8.5:  # RTX4060é™åˆ¶
                        optimizations[f'gpu_{i}_conservative'] = True
                        optimizations[f'gpu_{i}_max_batch'] = 16
                    else:
                        optimizations[f'gpu_{i}_conservative'] = False
                        optimizations[f'gpu_{i}_max_batch'] = 32
            
            # å†æ¬¡æ£€æŸ¥å†…å­˜
            memory_info_after = psutil.virtual_memory()
            optimizations['memory_after'] = {
                'total': memory_info_after.total / (1024**3),
                'available': memory_info_after.available / (1024**3),
                'percent': memory_info_after.percent
            }
            
            optimizations['memory_freed'] = optimizations['memory_after']['available'] - optimizations['memory_before']['available']
            
            logging.info(f"å†…å­˜ä¼˜åŒ–å®Œæˆ: {optimizations}")
            return optimizations
            
        except Exception as e:
            logging.error(f"å†…å­˜ä¼˜åŒ–å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def create_optimized_config(self) -> RAGConfig:
        """
        åˆ›å»ºä¼˜åŒ–åçš„é…ç½®
        
        Returns:
            ä¼˜åŒ–åçš„é…ç½®å¯¹è±¡
        """
        logging.info("åˆ›å»ºä¼˜åŒ–é…ç½®...")
        
        # è¿è¡Œæ‰€æœ‰ä¼˜åŒ–
        pytorch_opt = self.optimize_pytorch_settings()
        embedding_opt = self.optimize_embedding_model(self.config.EMBEDDING_MODEL_PATH)
        vector_opt = self.optimize_vector_store()
        text_opt = self.optimize_text_processing()
        memory_opt = self.optimize_memory_usage()
        
        # åˆ›å»ºæ–°é…ç½®
        optimized_config = RAGConfig()
        
        # åº”ç”¨æ–‡æœ¬å¤„ç†ä¼˜åŒ–
        if 'chunk_size' in text_opt:
            optimized_config.CHUNK_SIZE = text_opt['chunk_size']
        if 'chunk_overlap' in text_opt:
            optimized_config.CHUNK_OVERLAP = text_opt['chunk_overlap']
        
        # åº”ç”¨å‘é‡å­˜å‚¨ä¼˜åŒ–
        if 'batch_size' in vector_opt:
            optimized_config.BATCH_SIZE = vector_opt['batch_size']
        
        # åº”ç”¨æ€§èƒ½ä¼˜åŒ–
        optimized_config.MAX_RESPONSE_TIME = 2.0  # ç¡®ä¿2ç§’å†…å“åº”
        
        # ä¿å­˜ä¼˜åŒ–ä¿¡æ¯
        optimized_config.OPTIMIZATION_INFO = {
            'pytorch': pytorch_opt,
            'embedding': embedding_opt,
            'vector_store': vector_opt,
            'text_processing': text_opt,
            'memory': memory_opt,
            'hardware': self.hardware_info
        }
        
        logging.info("ä¼˜åŒ–é…ç½®åˆ›å»ºå®Œæˆ")
        return optimized_config
    
    def benchmark_system(self, config: RAGConfig = None) -> Dict[str, Any]:
        """
        ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•
        
        Args:
            config: é…ç½®å¯¹è±¡
            
        Returns:
            åŸºå‡†æµ‹è¯•ç»“æœ
        """
        logging.info("å¼€å§‹ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        config = config or self.config
        benchmark_results = {}
        
        try:
            # æµ‹è¯•æ–‡æœ¬å¤„ç†é€Ÿåº¦
            test_texts = [
                "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºè¯„ä¼°æ–‡æœ¬å¤„ç†æ€§èƒ½ã€‚" * 10,
                "This is a test text for evaluating text processing performance." * 10,
                "æ··åˆä¸­è‹±æ–‡æµ‹è¯• Mixed language test æ€§èƒ½è¯„ä¼° performance evaluation." * 10
            ]
            
            start_time = time.time()
            for text in test_texts * 10:  # é‡å¤æµ‹è¯•
                # æ¨¡æ‹Ÿæ–‡æœ¬åˆ†å—
                chunks = [text[i:i+config.CHUNK_SIZE] for i in range(0, len(text), config.CHUNK_SIZE - config.CHUNK_OVERLAP)]
            text_processing_time = time.time() - start_time
            
            benchmark_results['text_processing'] = {
                'time': text_processing_time,
                'texts_processed': len(test_texts) * 10,
                'speed': len(test_texts) * 10 / text_processing_time
            }
            
            # æµ‹è¯•å†…å­˜ä½¿ç”¨
            memory_before = psutil.virtual_memory().available / (1024**3)
            
            # æ¨¡æ‹Ÿå¤§é‡æ•°æ®å¤„ç†
            large_data = ["æµ‹è¯•æ•°æ®" * 1000] * 100
            processed_data = [data.lower() for data in large_data]
            
            memory_after = psutil.virtual_memory().available / (1024**3)
            memory_used = memory_before - memory_after
            
            benchmark_results['memory_usage'] = {
                'memory_before': memory_before,
                'memory_after': memory_after,
                'memory_used': memory_used,
                'data_processed': len(large_data)
            }
            
            # æ¸…ç†å†…å­˜
            del large_data, processed_data
            gc.collect()
            
            # GPUåŸºå‡†æµ‹è¯•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.hardware_info['gpu_available']:
                start_time = time.time()
                
                # æ¨¡æ‹ŸGPUè®¡ç®—
                test_tensor = torch.randn(1000, 1024).cuda()
                result = torch.matmul(test_tensor, test_tensor.T)
                torch.cuda.synchronize()
                
                gpu_time = time.time() - start_time
                
                benchmark_results['gpu_performance'] = {
                    'computation_time': gpu_time,
                    'tensor_size': test_tensor.shape,
                    'gpu_memory_used': torch.cuda.memory_allocated() / (1024**3)
                }
                
                # æ¸…ç†GPUå†…å­˜
                del test_tensor, result
                torch.cuda.empty_cache()
            
            # è®¡ç®—æ€»ä½“æ€§èƒ½è¯„åˆ†
            performance_score = self._calculate_performance_score(benchmark_results)
            benchmark_results['overall_score'] = performance_score
            
            logging.info(f"åŸºå‡†æµ‹è¯•å®Œæˆï¼Œæ€§èƒ½è¯„åˆ†: {performance_score:.2f}/100")
            return benchmark_results
            
        except Exception as e:
            logging.error(f"åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _calculate_performance_score(self, benchmark_results: Dict[str, Any]) -> float:
        """
        è®¡ç®—æ€§èƒ½è¯„åˆ†
        
        Args:
            benchmark_results: åŸºå‡†æµ‹è¯•ç»“æœ
            
        Returns:
            æ€§èƒ½è¯„åˆ† (0-100)
        """
        score = 0.0
        max_score = 100.0
        
        # æ–‡æœ¬å¤„ç†æ€§èƒ½ (30åˆ†)
        if 'text_processing' in benchmark_results:
            text_speed = benchmark_results['text_processing']['speed']
            if text_speed >= 100:
                score += 30
            elif text_speed >= 50:
                score += 20
            elif text_speed >= 20:
                score += 15
            else:
                score += 10
        
        # å†…å­˜ä½¿ç”¨æ•ˆç‡ (30åˆ†)
        if 'memory_usage' in benchmark_results:
            memory_used = benchmark_results['memory_usage']['memory_used']
            if memory_used <= 1.0:  # å°äº1GB
                score += 30
            elif memory_used <= 2.0:  # å°äº2GB
                score += 25
            elif memory_used <= 4.0:  # å°äº4GB
                score += 20
            else:
                score += 10
        
        # GPUæ€§èƒ½ (20åˆ†)
        if 'gpu_performance' in benchmark_results:
            gpu_time = benchmark_results['gpu_performance']['computation_time']
            if gpu_time <= 0.1:
                score += 20
            elif gpu_time <= 0.5:
                score += 15
            elif gpu_time <= 1.0:
                score += 10
            else:
                score += 5
        else:
            score += 10  # CPUæ¨¡å¼åŸºç¡€åˆ†
        
        # ç¡¬ä»¶é…ç½® (20åˆ†)
        if self.hardware_info['memory_total'] >= 20:
            score += 10
        elif self.hardware_info['memory_total'] >= 15:
            score += 8
        else:
            score += 5
        
        if self.hardware_info['gpu_available']:
            score += 10
        else:
            score += 5
        
        return min(score, max_score)
    
    def generate_optimization_report(self) -> Dict[str, Any]:
        """
        ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
        
        Returns:
            ä¼˜åŒ–æŠ¥å‘Š
        """
        logging.info("ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š...")
        
        # åˆ›å»ºä¼˜åŒ–é…ç½®
        optimized_config = self.create_optimized_config()
        
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        benchmark_results = self.benchmark_system(optimized_config)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
            'hardware_info': self.hardware_info,
            'optimization_applied': optimized_config.OPTIMIZATION_INFO,
            'benchmark_results': benchmark_results,
            'recommendations': self._generate_recommendations(benchmark_results),
            'config_changes': self._get_config_changes(optimized_config)
        }
        
        return report
    
    def _generate_recommendations(self, benchmark_results: Dict[str, Any]) -> List[str]:
        """
        ç”Ÿæˆä¼˜åŒ–å»ºè®®
        
        Args:
            benchmark_results: åŸºå‡†æµ‹è¯•ç»“æœ
            
        Returns:
            ä¼˜åŒ–å»ºè®®åˆ—è¡¨
        """
        recommendations = []
        
        # åŸºäºç¡¬ä»¶çš„å»ºè®®
        if self.hardware_info['memory_total'] < 16:
            recommendations.append("å»ºè®®å¢åŠ ç³»ç»Ÿå†…å­˜åˆ°16GBä»¥ä¸Šä»¥è·å¾—æ›´å¥½æ€§èƒ½")
        
        if not self.hardware_info['gpu_available']:
            recommendations.append("å»ºè®®ä½¿ç”¨GPUåŠ é€Ÿä»¥æå‡åµŒå…¥è®¡ç®—æ€§èƒ½")
        elif self.hardware_info['gpu_memory'] and self.hardware_info['gpu_memory'][0]['memory_total'] <= 8.5:
            recommendations.append("å½“å‰GPUå†…å­˜æœ‰é™(8GB)ï¼Œå»ºè®®ä½¿ç”¨æ¨¡å‹é‡åŒ–å’Œæ‰¹å¤„ç†ä¼˜åŒ–")
        
        # åŸºäºæ€§èƒ½æµ‹è¯•çš„å»ºè®®
        if 'overall_score' in benchmark_results:
            score = benchmark_results['overall_score']
            if score < 60:
                recommendations.append("ç³»ç»Ÿæ€§èƒ½è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥ç¡¬ä»¶é…ç½®å’Œä¼˜åŒ–è®¾ç½®")
            elif score < 80:
                recommendations.append("ç³»ç»Ÿæ€§èƒ½ä¸­ç­‰ï¼Œå¯é€šè¿‡è°ƒæ•´æ‰¹å¤„ç†å¤§å°å’Œç¼“å­˜ç­–ç•¥è¿›ä¸€æ­¥ä¼˜åŒ–")
            else:
                recommendations.append("ç³»ç»Ÿæ€§èƒ½è‰¯å¥½ï¼Œå½“å‰ä¼˜åŒ–è®¾ç½®é€‚åˆæ‚¨çš„ç¡¬ä»¶é…ç½®")
        
        # å†…å­˜ä½¿ç”¨å»ºè®®
        if 'memory_usage' in benchmark_results:
            memory_used = benchmark_results['memory_usage']['memory_used']
            if memory_used > 4.0:
                recommendations.append("å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œå»ºè®®å‡å°‘æ‰¹å¤„ç†å¤§å°æˆ–å¯ç”¨ä¿å®ˆæ¨¡å¼")
        
        # æ–‡æœ¬å¤„ç†å»ºè®®
        if 'text_processing' in benchmark_results:
            speed = benchmark_results['text_processing']['speed']
            if speed < 20:
                recommendations.append("æ–‡æœ¬å¤„ç†é€Ÿåº¦è¾ƒæ…¢ï¼Œå»ºè®®å¯ç”¨å¹¶è¡Œå¤„ç†æˆ–å‡å°‘åˆ†å—å¤§å°")
        
        return recommendations
    
    def _get_config_changes(self, optimized_config: RAGConfig) -> Dict[str, Any]:
        """
        è·å–é…ç½®å˜æ›´
        
        Args:
            optimized_config: ä¼˜åŒ–åçš„é…ç½®
            
        Returns:
            é…ç½®å˜æ›´ä¿¡æ¯
        """
        original_config = RAGConfig()
        
        changes = {}
        
        if optimized_config.CHUNK_SIZE != original_config.CHUNK_SIZE:
            changes['CHUNK_SIZE'] = {
                'original': original_config.CHUNK_SIZE,
                'optimized': optimized_config.CHUNK_SIZE
            }
        
        if optimized_config.CHUNK_OVERLAP != original_config.CHUNK_OVERLAP:
            changes['CHUNK_OVERLAP'] = {
                'original': original_config.CHUNK_OVERLAP,
                'optimized': optimized_config.CHUNK_OVERLAP
            }
        
        if optimized_config.BATCH_SIZE != original_config.BATCH_SIZE:
            changes['BATCH_SIZE'] = {
                'original': original_config.BATCH_SIZE,
                'optimized': optimized_config.BATCH_SIZE
            }
        
        return changes

def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        optimizer = PerformanceOptimizer()
        
        # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
        report = optimizer.generate_optimization_report()
        
        # æ‰“å°æŠ¥å‘Š
        logging.info("\n" + "=" * 60)
        logging.info("RAGç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–æŠ¥å‘Š")
        logging.info("=" * 60)
        
        # ç¡¬ä»¶ä¿¡æ¯
        hardware = report['hardware_info']
        logging.info(f"\nğŸ’» ç¡¬ä»¶é…ç½®:")
        logging.info(f"  CPUæ ¸å¿ƒæ•°: {hardware['cpu_count']}")
        logging.info(f"  æ€»å†…å­˜: {hardware['memory_total']:.1f}GB")
        logging.info(f"  å¯ç”¨å†…å­˜: {hardware['memory_available']:.1f}GB")
        logging.info(f"  GPUå¯ç”¨: {'æ˜¯' if hardware['gpu_available'] else 'å¦'}")
        if hardware['gpu_available']:
            for gpu in hardware['gpu_memory']:
                logging.info(f"  GPU {gpu['device']}: {gpu['name']} ({gpu['memory_total']:.1f}GB)")
        
        # æ€§èƒ½è¯„åˆ†
        if 'overall_score' in report['benchmark_results']:
            score = report['benchmark_results']['overall_score']
            logging.info(f"\nğŸ“Š æ€§èƒ½è¯„åˆ†: {score:.1f}/100")
        
        # ä¼˜åŒ–å»ºè®®
        recommendations = report['recommendations']
        if recommendations:
            logging.info(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for i, rec in enumerate(recommendations, 1):
                logging.info(f"  {i}. {rec}")
        
        # é…ç½®å˜æ›´
        config_changes = report['config_changes']
        if config_changes:
            logging.info(f"\nâš™ï¸ é…ç½®ä¼˜åŒ–:")
            for key, change in config_changes.items():
                logging.info(f"  {key}: {change['original']} â†’ {change['optimized']}")
        
        logging.info("\n" + "=" * 60)
        logging.info("ä¼˜åŒ–å®Œæˆï¼å»ºè®®ä½¿ç”¨ä¼˜åŒ–åçš„é…ç½®è¿è¡ŒRAGç³»ç»Ÿã€‚")
        logging.info("=" * 60)
        
        return True
        
    except Exception as e:
        logging.error(f"æ€§èƒ½ä¼˜åŒ–å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    main()